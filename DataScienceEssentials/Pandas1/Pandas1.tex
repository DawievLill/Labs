\lab{Pandas 1: Introduction}{Pandas 1: Introduction}
\objective{Though NumPy and SciPy are powerful tools for numerical computing, they lack some of the high-level functionality necessary for many data science applications.
Python's \emph{pandas} library, built on NumPy, is designed specifically for data management and analysis.
In this lab, we introduce pandas data structures, syntax, and explore its capabilities for quickly analyzing and presenting data.
}
\label{lab:pandas1}

% Just as NumPy is built on the \li{ndarray} data structure suited for efficient
% scientific and numerical computation, pandas is centered around a handful of
% core data structures custom built for data analysis: the \li{Series}, \li{DataFrame}, and \li{Panel}, which roughly correspond to one, two, and three-dimensional arrays.
% The \li{Panel} data structure is not used nearly

\begin{info}
This lab will be done using Colab Notebooks.
These notebooks are similar to Jupyter Notebooks but run remotely on Google's servers.
Open a Google Colab notebook by going to your Google Drive account and creating a new Colaboratory file.
If making a Colaboratory file is not an option, download the application Colaboratory onto your Google Drive.
Once opening a new Colab Notebook, upload the file \li{pandas1.ipynb}.
To make the data files accessible, run the following at the top of the lab:
\begin{lstlisting}
>>> from google.colab import files

>>> uploaded = files.upload()
\end{lstlisting}
This will prompt you upload files for this notebook.
For this lab, upload \li{budget.csv} and \li{crime_data.csv}.

Once the lab is complete, delete BOTH lines of code used for uploading files (the import statement and the upload statement) and download as a \li{.py} file to your git repository.
Push the newly made \li{pandas1.py} file.
\end{info}

\section*{Pandas Basics}

Pandas is a python library used primarily to analyze data.
It combines functionality of NumPy, MatPlotLib, and SQL to create a easy to understand library that allows for the manipulation of data in various ways.
In this lab, we focus on the use of Pandas to analyze and manipulate data in ways similar to NumPy and SQL.

\subsection*{Pandas Data Structures}

\subsubsection*{Series}

The first pandas data structure is a \li{Series}.
A \li{Series} is a one-dimensional array that can hold any datatype, similar to a \li{ndarray}.
However, a \li{Series} has an \li{index} that gives a label to each entry.
An \li{index} generally is used to label the data.

Typically a \li{Series} contains information about one feature of the data.
For example, the data in a \li{Series} might show a class's grades on a test and the \li{Index} would indicate each student in the class.
To initialize a \li{Series}, the first parameter is the data and the second is the index.

\begin{lstlisting}
>>> import pandas as pd
>>>
>>> # Initialize Series of student grades
>>> math = pd.Series(np.random.randint(0,100,4), ['Mark', 'Barbara', 'Eleanor', 'David'])
>>> english = pd.Series(np.random.randint(0,100,5), ['Mark', 'Barbara', 'David', 'Greg', 'Lauren'])
\end{lstlisting}

\subsubsection*{DataFrame}

The second key pandas data structure is a \li{DataFrame}.
A \li{DataFrame} is a collection of multiple \li{Series}.
It can be thought of as a 2-dimensional array, where each row is a separate datapoint and each column is a feature of the data.
The rows are label with an \li{index} (as in a \li{Series}) and the columns are labelled in the attribute \li{columns}.

There are many different ways to initialize a \li{DataFrame}.
One way to initialize a \li{DataFrame} is passing in a dictionary as the data of the \li{DataFrame}.
The keys of the dictionary will become the labels in \li{columns} and the values are the \li{Series} associated with the label.

\begin{lstlisting}
>>> # Create a DataFrame of student grades
>>> grades = pd.DataFrame({"Math": math, "English": english}
>>> grades
	      Math  English
Barbara   52.0     73.0
David     10.0     39.0
Eleanor   35.0      NaN
Greg       NaN     26.0
Lauren     NaN     99.0
Mark      81.0     68.0
\end{lstlisting}

Notice that \li{pd.DataFrame} automatically lines up data from both \li{Series} that have the same index.
If the data only appears in one of the \li{Series}, the entry for the second \li{Series} is \li{NaN}.

We can also initialize a \li{DataFrame} with a NumPy array.
In this way, the data is passed in as a 2-dimensional NumPy array, while the column labels and index are passed in as parameters.
The first column label goes with the first column of the array, the second with the second, etc.
The same holds for the index.

\begin{lstlisting}
>>> import numpy as np
>>> # Initialize DataFrame with NumPy array
>>> data = np.array([[52.0, 73.0], [10.0, 39.0], [35.0, np.nan], [np.nan, 26.0], [np.nan, 99.0], [81.0, 68.0]])
>>> grades = pd.DataFrame(data, columns = ['Math', 'English'], index = ['Barbara', 'David', 'Eleanor', 'Greg', 'Lauren', 'Mark'])
>>> grades
	      Math  English
Barbara   52.0     73.0
David     10.0     39.0
Eleanor   35.0     NaN
Greg       NaN     26.0
Lauren     NaN     99.0
Mark      81.0     68.0
\end{lstlisting}

A \li{DataFrame} can also be viewed as a NumPy array using the attribute \li{values}.

\begin{lstlisting}
>>> # View the DataFrame as a NumPy array
>>> grades.values
array([[ 52.,  73.],
       [ 10.,  39.],
       [ 35.,  nan],
       [ nan,  26.],
       [ nan,  99.],
       [ 81.,  68.]])
\end{lstlisting}

\begin{problem}
Write a function \li{random_dataframe()} that accepts a dictionary \li{d} which defaults to \li{None}.
If a dictionary is passed in, initialize a Pandas \li{DataFrame}.
Return a tuple of the attributes \li{index}, \li{columns}, and \li{values} of the \li{DataFrame}.

If a dictionary is not passed in, generate random data as a \li{ndarray} and initialize a \li{DataFrame}.
The columns of the \li{DataFrame} should be the letters \li{'A'} through \li{'E'}.
The index of the \li{DataFrame} should be the roman numerals 1-6.
Return a tuple of the attributes \li{index}, \li{columns}, and \li{values} of the \li{DataFrame}.

(Hint: What should the dimension of the data be if no dictionary is passed in?)
\end{problem}

\begin{comment}
The Pandas python library is used to manipulate and analyze data.
The two main pandas structures are \li{Series} and \li{DataFrames}.
They are built off of \li{ndarrays} and thereby carry many functionalities of arrays.
A \li{Series} is a one-dimensional array that can hold any datatype.
Unlike a NumPy array, every \li{Series} has an \emph{index} that labels each entry, and a \li{Series} object can also be given a name to label the entire data set.
A group of \li{Series} makes a 2-d structure called a \li{DataFrame}.
A \li{DataFrame} is like a table while the \li{Series} are the columns.
The row labels are collectively called the \emph{index}, and the column labels are collectively called the \emph{columns}.
There are many ways to initialize a \li{DataFrame}.
In the following code, we build a \li{DataFrame} out of a dictionary of \li{Series}.

\begin{lstlisting}
>>> x = pd.Series(np.random.randn(4), ['a', 'b', 'c', 'd'])
>>> y = pd.Series(np.random.randn(5), ['a', 'b', 'd', 'e', 'f'])
>>> d = {"series 1": x, "series 2": y}
>>> df1 = pd.DataFrame(d)
>>> df1
<<   series 1  series 2
a -0.365542  1.227960
b  0.080133  0.683523
c  0.737970       NaN
d  0.097878 -1.102835
e       NaN  1.345004
f       NaN  0.217523>>
\end{lstlisting}

The index of this \li{DataFrame} is the union of the indices of \li{Series} \li{x} and \li{Series} \li{y}.
The columns are given by the keys of the dictionary \li{d}.
Since \li{x} doesn't have a label \li{e}, the value in row \li{e}, column \li{1} is \li{NaN}.
This same reasoning explains the other missing values as well.
We can recover the \li{Series} \li{x} by slicing using the axis label for the first column on the \li{DataFrame} and \li{dropna()}, which removes the \li{NaN}:
 \begin{lstlisting}
>>> df1["series1"].dropna()
<<a   -0.365542
b    0.080133
c    0.737970
d    0.097878
Name: series 1, dtype: float64>>
\end{lstlisting}

\begin{warn}
A pandas \li{DataFrame} cannot be sliced in exactly the same way as a NumPy array.
Notice how we just used \li{df1["series 1"]} to access a \emph{column} of the
the \li{DataFrame} \li{df1}.
We will discuss this in more detail later on.
\end{warn}

We can also initialize a \li{DataFrame} using a NumPy array, creating custom
row and column labels.

\begin{lstlisting}
>>> data = np.random.random((3, 4))
>>> pd.DataFrame(data, index=['A', 'B', 'C'], columns=np.arange(1, 5))
             1         2         3         4
A  0.065646  0.968593  0.593394  0.750110
B  0.803829  0.662237  0.200592  0.137713
C  0.288801  0.956662  0.817915  0.951016
3 rows     4 columns
\end{lstlisting}

If we don't specify the index or columns, the default is
\li{np.arange(n)}, where \li{n} is either the number of rows or columns.
\end{comment}

\subsection*{Data I/O}

The pandas library has functions that make importing and exporting data simple.
The functions allow for a variety of file formats to be imported and exported, including CSV, Excel, HDF5, SQL, JSON, HTML, and pickle files.

\begin{table}[H]
\begin{tabular}{r|l}
Method & Description \\ \hline
\begin{comment}
\li{describe()}  & Return a \li{Series} describing the data structure \\
\li{head()}      & Return the first $n$ rows, defaulting to 5 \\
\li{tail()}      & Return the last $n$ rows, defaulting to 5 \\
\end{comment}
\li{to_csv()}    & Write the index and entries to a CSV file \\
\li{to_json()}   & Convert the object to a JSON string \\
\li{to_pickle()} & Serialize the object and store it in an external file \\
\li{to_sql()}    & Write the object data to an open SQL database \\
\end{tabular}
\caption{Methods for exporting data in a pandas \li{Series} or \li{DataFrame}.}
\label{table:pandas-view-or-export}
\end{table}

The CSV (comma separated values) format is a simple way of storing tabular data
in plain text.
Because CSV files are one of the most popular file formats for
exchanging data, we will explore the \li{read_csv()} function in more detail.
To learn to read other types of file formats, see the online pandas
documentation.
To read a CSV data file into a \li{DataFrame}, call the
\li{read_csv()} function with the path to the CSV file, along with the
appropriate keyword arguments.
Below we list some of the most important keyword
arguments:

\begin{itemize}
\item \li{delimiter}:
The character that separates data fields. It is often a
comma or a whitespace character.
\item \li{header}:
The row number (0 indexed) in the CSV file that contains the column names.
 \item \li{index_col}:
The column (0 indexed) in the CSV file that is the index for the
\li{DataFrame}.
 \item \li{skiprows}:
If an integer $n$, skip the first $n$ rows of the file, and then start reading
in the data.
If a list of integers, skip the specified rows.
 \item \li{names}:
If the CSV file does not contain the column names, or you wish to use other
column names, specify them in a list.
\end{itemize}

\begin{comment}
\begin{problem}
Write a function \li{prob2()} that accepts a filename.
Check whether the file is in CSV format and read the data in appropriately.
If the file is not in CSV format, raise a \li{TypeError}.


\label{prob:read-csv}
\end{problem}

To save data, pandas has functions that write to several different
file formats.
A typical example is the \li{to_csv()} function method attached to
\li{Series} and \li{DataFrame} objects, which writes the data to a CSV file.
Keyword arguments allow you to specify the separator character, omit writing the
columns names or index, and specify many other options.
The code below
demonstrates its typical usage:
\begin{lstlisting}
>>> df.to_csv("my_df.csv")
\end{lstlisting}
\end{comment}



\section*{Data Manipulation}

\subsection*{Accessing Data}

While array slicing can be used to access data in a \li{DataFrame}, it is always preferable to use the \li{loc} and \li{iloc} indexers.
Accessing \li{Series} and \li{DataFrame} objects
using these indexing operations is more efficient than slicing because the bracket indexing has to check many cases before it can determine how to slice the data structure.
Using \li{loc}/\li{iloc} explicitly, bypasses the extra checks.
The \li{loc} index selects rows and columns based on
their labels, while \li{iloc} selects them based on their
integer position.
When using these indexers, the first and second arguments refer to the rows and columns, respectively, just as array slicing.

\begin{lstlisting}
>>> grades
	      Math  English
Barbara   52.0     73.0
David     10.0     39.0
Eleanor   35.0     NaN
Greg       NaN     26.0
Lauren     NaN     99.0
Mark      81.0     68.0

>>> # Use loc to select the Math scores of David and Greg
>>> grades.loc[['David', 'Greg'],'Math']
David    10.0
Greg      NaN
Name: Math, dtype: float64

>>> # Use iloc to select the Math scores of David and Greg
>>> grades.iloc[[1,3], 0]
David    10.0
Greg      NaN
\end{lstlisting}

An entire column of a \li{DataFrame} can be accessed using simple square
brackets and the name of the column.
In addition, to create a new column or reset the values of an entire column, simply call this column in the same fashion and set the value.

\begin{lstlisting}
>>> # Set new History column with array of random values
>>> grades['History'] = np.random.randint(0,100,6)
>>> grades['History']
Barbara     4
David      92
Eleanor    25
Greg       79
Lauren     82
Mark       27
Name: History, dtype: int64

>>> # Reset the column such that everyone has a 100
>>> grades['History'] = 100.0
>>> grades
         Math  English  History
Barbara  52.0     73.0    100.0
David    10.0     39.0    100.0
Eleanor  35.0      NaN    100.0
Greg      NaN     26.0    100.0
Lauren    NaN     99.0    100.0
Mark     81.0     68.0    100.0
\end{lstlisting}


Often datasets can be very large and difficult to visualize.
Pandas offers various methods to make the data easier to visualize.
The methods \li{head} and \li{tail} will show the first or last $n$ data points, respectively, where $n$ defaults to 5.
The method \li{sample} will draw $n$ random entry of the dataset, where $n$ defaults to 1.

\begin{lstlisting}
>>> # Use head to see the first n rows
>>> grades.head(n=2)
         Math  English  History
Barbara  52.0     73.0    100.0
David    10.0     39.0    100.0

>>> # Use sample to sample a random entry
>>> grades.sample()
        Math  English  History
Lauren   NaN     99.0    100.0
\end{lstlisting}

It may also be useful to re-order the columns or rows or sort according to a
given column.

\begin{lstlisting}
>>> # Re-order columns
>>> grades.reindex(columns['English','Math','History'])
         English  Math  History
Barbara     73.0  52.0    100.0
David       39.0  10.0    100.0
Eleanor      NaN  35.0    100.0
Greg        26.0   NaN    100.0
Lauren      99.0   NaN    100.0
Mark        68.0  81.0    100.0

>>> # Sort descending according to Math grades
>>> grades.sort_values('Math', ascending=False)
         Math  English  History
Mark     81.0     68.0    100.0
Barbara  52.0     73.0    100.0
Eleanor  35.0      NaN    100.0
David    10.0     39.0    100.0
Greg      NaN     26.0    100.0
Lauren    NaN     99.0    100.0
\end{lstlisting}

Other methods used for manipulating \li{DataFrame} and \li{Series} panda structures can be found in Table \ref{table:pandas-manage-data}.

\begin{table}[H]
\begin{tabular}{r|l}
Method & Description \\ \hline
\li{append()} & Concatenate two or more \li{Series}. \\
\li{drop()} & Remove the entries with the specified label or labels \\
\li{drop_duplicates()} & Remove duplicate values \\
\li{dropna()} & Drop null entries \\
\li{fillna()} & Replace null entries with a specified value or strategy \\
\li{reindex()} & Replace the index \\
\li{sample()} & Draw a random entry \\
\li{shift()} & Shift the index \\
% \li{truncate()} & \\
\li{unique()} & Return unique values \\
% \li{where()} & Search for entires that satisfy some criteria \\
\end{tabular}
\caption{Methods for managing or modifying data in a pandas \li{Series} or \li{DataFrame}.}
\label{table:pandas-manage-data}
\end{table}

\begin{problem}
The file \li{budget.csv} contains the budget of a college student over the course of 4 years.
Write a function \li{prob2()} reads in \li{budget.csv} as a \li{DataFrame}.
Perform the following operations:
\begin{enumerate}
\item Reindex the columns such that amount spent on food is the first column and all other columns maintain the same ordering.
\item Sort the \li{DataFrame} in descending order based on how much money was spent on \li{Groceries}
\item Reset all values in the \li{'Rent'} column to \li{800.0}
\item Reset all values in the first 5 data points to \li{0.0}
\end{enumerate}
Return the values of the updated \li{DataFrame} as a NumPy array.
\label{prob:budget}
\end{problem}

\subsection*{Basic Data Manipulation}
Because the primary pandas data structures are subclasses of \li{ndarray}, most NumPy functions work with pandas structure.
For example, basic
vector operations work as expected:

\begin{lstlisting}
>>> # Sum history and english grades of all students
>>> grades['English'] + grades['History']
Barbara    173.0
David      139.0
Eleanor      NaN
Greg       126.0
Lauren     199.0
Mark       168.0
dtype: float64

>>> # Double all Math grades
>>>  grades['Math']*2
Barbara    104.0
David       20.0
Eleanor     70.0
Greg         NaN
Lauren       NaN
Mark       162.0
Name: Math, dtype: float64
\end{lstlisting}
In addition to arithmetic, \li{Series} have a variety of other methods similar to NumPy arrays.
A collection of these methods is found in Table \ref{table:pandas-numerical-methods}.
\begin{table}[H]
\begin{tabular}{r|l}
Method & Returns \\ \hline
\li{<<abs>>()}     & Object with absolute values taken (of numerical data) \\
\li{idxmax()}  & The index label of the maximum value \\
\li{idxmin()}  & The index label of the minimum value \\
\li{count()}   & The number of non-null entries \\
\li{cumprod()} & The cumulative product over an axis \\
\li{cumsum()}  & The cumulative sum over an axis \\
\li{<<max>>()}     & The maximum of the entries \\
\li{mean()}    & The average of the entries \\
\li{median()}  & The median of the entries \\
\li{<<min>>()}     & The minimum of the entries \\
\li{mode()}    & The most common element(s) \\
\li{prod()}    & The product of the elements \\
\li{<<sum>>()}     & The sum of the elements \\
\li{var()}     & The variance of the elements \\
\end{tabular}
\caption{Numerical methods of the \li{Series} and \li{DataFrame} pandas classes.
% Methods marked with a * are not methods of NumPy's \li{ndarray} class.
}
\label{table:pandas-numerical-methods}
\end{table}

%The default
% missing value \li{NaN} is given for labels that are not shared by both inputs.

\subsection*{Basic Statistical Functions}

The pandas library allows us to easily calculate basic summary statistics of our data,
useful when we want a quick description of the data.
The \li{describe()} function
outputs several such summary statistics for each column in a \li{DataFrame}:
\begin{lstlisting}
>>> # Use describe to better understand the data
>>> grades.describe()
            Math   English  History
count   4.000000   5.00000      6.0
mean   44.500000  61.00000    100.0
std    29.827281  28.92231      0.0
min    10.000000  26.00000    100.0
25%    28.750000  39.00000    100.0
50%    43.500000  68.00000    100.0
75%    59.250000  73.00000    100.0
max    81.000000  99.00000    100.0
\end{lstlisting}

Functions for calculating means and variances, the covariance and correlation matrices, and other
basic statistics are also available.

\begin{lstlisting}
>>> # Find the average grade for each student
>>> grades.mean(axis=1)
Barbara    75.000000
David      49.666667
Eleanor    67.500000
Greg       63.000000
Lauren     99.500000
Mark       83.000000
dtype: float64

>>> # Solve for the unbiased variance between subjects
>>> grades.cov()
               Math  English  History
Math     889.666667    557.0      0.0
English  557.000000    836.5      0.0
History    0.000000      0.0      0.0

>>> # Give correlation matrix between subjects
>>> grades.corr()
            Math  English  History
Math     1.00000  0.84996      NaN
English  0.84996  1.00000      NaN
History      NaN      NaN      NaN
\end{lstlisting}

The method \li{rank} gives a ranking based on methods such as average, minimum, and maximum.
This method defaults ranking in ascending order (the least will be ranked 1 and the greatest will be ranked the highest number).

\begin{lstlisting}
>>> # Rank each student's performance based on their highest grade in any class in descending order
>>> grades.rank(axis=1,method='max',ascending=False)
         Math  English  History
Barbara   3.0      2.0      1.0
David     3.0      2.0      1.0
Eleanor   2.0      NaN      1.0
Greg      NaN      2.0      1.0
Lauren    NaN      2.0      1.0
Mark      2.0      3.0      1.0
\end{lstlisting}

These methods can be very effective in interpreting data.
For example, the \li{rank} example above shows use that Barbara does best in History, then English and then Math.

\subsection*{Dealing with Missing Data}

Missing data is a ubiquitous problem in data science.
Fortunately, pandas is particularly well-suited to
handling missing and anomalous data.
As we have already seen, the pandas default for a missing value is \li{NaN}.
In basic arithmetic operations, if one of the operands is \li{NaN}, then the output is also \li{NaN}.
If we are not interested in the missing values, we can simply drop them from the data altogether:

\begin{lstlisting}
>>> # Grades with all NaN values dropped
>>> grades.dropna()
         Math  English  History
Barbara  52.0     73.0    100.0
David    10.0     39.0    100.0
Mark     81.0     68.0    100.0
\end{lstlisting}

This is not always the desired behavior, however.
Missing data could actually correspond to some default value, such as zero.
For example, in the budget dataset, filling \li{NaN} value with 0 indicates that no money was spent on that item.
In the grade dataset, we can replace all instances of \li{NaN} with a specified value:
\begin{lstlisting}
>>> # fill missing data with 50.0
>>> grades.fillna(50.0)
         Math  English  History
Barbara  52.0     73.0    100.0
David    10.0     39.0    100.0
Eleanor  35.0     50.0    100.0
Greg     50.0     26.0    100.0
Lauren   50.0     99.0    100.0
Mark     81.0     68.0    100.0
\end{lstlisting}

When dealing with missing data, make sure you are aware of the behavior of the pandas functions you are using.
For example, \li{<<sum()>>} and \li{mean()} ignore NaN values in the computation.

\begin{warn}
Always consider missing data carefully when analyzing a dataset.
It may not always be helpful to drop the data or fill it in with a random number.
Consider filling the data with the mean of surrounding data or the mean of the feature in question.
Overall, the choice for how to fill missing data should make sense with the dataset.
\end{warn}

\begin{problem}
Write a function \li{prob3()} that uses \li{budget.csv} to answer the questions "Which category affects living expenses the most? Which affects other expenses the most? How much is generally spent in these two categories?".
Use the functions above to manipulate the data to perform the following manipulations:
\begin{enumerate}
\item Fill all \li{NaN} values with \li{0.0}.
\item Create two new columns, \li{'Living Expenses'} and \li{'Other'}. 
Sum the columns \li{'Rent', 'Groceries', 'Gas'} and \li{'Utilities'} and set as the value of \li{'Living Expenses'}.
Sum the columns \li{'Dining Out', 'Out With Friends'} and \li{'Netflix'} and set as the value of \li{'Other'}.
\item  Identify which column correlates most with \li{'Living Expenses'} and which correlates most with \li{'Other'}.
This can indicate which columns in the budget affects the overarching categories the most.
\end{enumerate}
Return the mean of each the two columns found in 3 as a tuple.
The first mean should be of the column corresponding to \li{'Living Expenses'} and the second to \li{'Other'}.
\end{problem}

\begin{comment}
\begin{problem}
Read in the file \li{crime_data.csv} as a pandas object.
The file contains data on types of crimes in the U.S. from 1960 to 2016.
Set the index as the column 'Year'.

Create a new column \li{Rate} which contains the crime rate for each year.
Use this table to answer the following questions:
\begin{enumerate}
    \item What is the mean crime rate? In what years was the crime rate above the mean?
    \item On average, using the mean, what is the least common crime?
    \item Identify two crimes which have close means. Do these crimes have any kind of correlation from year to year? If so, what is the correlation?
       
\end{enumerate}
\end{problem}
\end{comment}

\begin{comment}
\subsection*{Series}

A pandas \li{Series} is generalization of a one-dimensional NumPy array, or the column of a DataFrame.
Like a NumPy array, every \li{Series} has a data type (\li{dtype}), and the entries of the \li{Series} are all of that type.
Unlike a NumPy array, every \li{Series} has an \emph{index} that labels each entry, and a \li{Series} object can also be given a name to label the entire data set.

\begin{lstlisting}
>>> import numpy as np
>>> import pandas as pd

# Initialize a Series of random entries with an index of letters.
>>> pd.Series(np.random.random(4), index=['a', 'b', 'c', 'd'])
<<a    0.474170
b    0.106878
c    0.420631
d    0.279713
dtype: float64>>

# The default index is integers from 0 to the length of the data.
>>> pd.Series(np.random.random(4), name="uniform draws")
<<0    0.767501
1    0.614208
2    0.470877
3    0.335885
Name: uniform draws, dtype: float64>>
\end{lstlisting}

The index in a \li{Series} is a pandas object of type \li{Index} and is stored as the \li{index} attribute of the \li{Series}.
The plain entries in the \li{Series} are stored as a NumPy array and can be accessed as such via the \li{values} attribute.

\begin{lstlisting}
>>> s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'], name="some ints")

>>> s1.values                           # Get the entries as a NumPy array.
<<array([1, 2, 3, 4])>>

>>> print(s1.name, s1.dtype, sep=", ")  # Get the name and dtype.
<<some ints, int64>>

>>> s1.index                            # Get the pd.Index object.
<<Index(['a', 'b', 'c', 'd'], dtype='object')>>
\end{lstlisting}

The elements of a \li{Series} can be accessed by either the regular position-based integer index, or by the corresponding label in the index.
New entries can be added dynamically as long as a valid index label is provided, similar to adding a new key-value pair to a dictionary.
A \li{Series} can also be initialize from a dictionary: the keys become the index labels, and the values become the entries.

\begin{lstlisting}
>>> s2 = pd.Series([10, 20, 30], index=["apple", "banana", "carrot"])
>>> s2
<<apple     10
banana    20
carrot    30
dtype: int64>>

# s2[0] and s2["apple"] refer to the same entry.
>>> print(s2[0], s2["apple"], s2["carrot"])
<<10 10 30>>

>>> s2[0] += 5              # Change the value of the first entry.
>>> s2["dewberry"] = 0      # Add a new value with label 'dewberry'.
>>> s2
<<apple       15
banana      20
carrot      30
dewberry     0
dtype: int64>>

# Initialize a Series from a dictionary.
>>> pd.Series({"eggplant":3, "fig":5, "grape":7}, name="more foods")
<<eggplant    3
fig         5
grape       7
Name: more foods, dtype: int64>>
\end{lstlisting}

Slicing and fancy indexing also work the same way in \li{Series} as in NumPy arrays.
In addition, multiple entries of a \li{Series} can be selected by indexing a list of labels in the index.

\begin{lstlisting}
>>> s3 = pd.Series({"lions":2, "tigers":1, "bears":3}, name="oh my")
>>> s3
<<bears     3
lions     2
tigers    1
Name: oh my, dtype: int64>>

# Get a subset of the data by regular slicing.
>>> s3[1:]
<<lions     2
tigers    1
Name: oh my, dtype: int64>>

# Get a subset of the data with fancy indexing.
>>> s3[np.array([len(i) == 5 for i in s3.index])]
<<bears    3
lions    2
Name: oh my, dtype: int64>>

# Get a subset of the data by providing several index labels.
>>> s3[ ["tigers", "bears"] ]
tigers    1                     # Note that the entries are reordered,
bears     3                     # and the name stays the same.
<<Name: oh my, dtype: int64>>
\end{lstlisting}

\begin{comment} % Unnecessary, already getting too long.
We can create a \li{Series} having constant values in the following manner:
\begin{lstlisting}
>>> val = 4     #desired constant value of Series
>>> n = 6       #desired length of Series
>>> s3 = pd.Series(val, index=np.arange(n))
>>> s3
0    4
1    4
2    4
3    4
4    4
5    4
dtype: int64
\end{lstlisting}


\begin{problem}
Create a pandas \li{Series} where the index labels are the even integers $0,2,\ldots,50$, and the entries are $n^2 - 1$, where $n$ is the entry's label.
Set all of the entries equal to zero whose labels are divisible by $3$.
\end{problem}


\subsection*{Data Manipulation with Series} % ----------------------------------------

A \li{Series} object, or column of a \li{DataFrame}, has all of the advantages of a NumPy array, including entry-wise arithmetic, plus a few additional features (see Table \ref{table:pandas-numerical-methods}).


\begin{lstlisting}
>>> s4 = pd.Series([1, 2, 4], index=['a', 'c', 'd'])
>>> s5 = pd.Series([10, 20, 40], index=['a', 'b', 'd'])
>>> 2*s4 + s5
<<a    12.0>>
<<b     NaN>>                           # s4 doesn't have an entry for b, and
<<c     NaN>>                           # s5 doesn't have an entry for c, so
<<d    48.0>>                           # the combination is Nan (np.nan / None).
<<dtype: float64>>
\end{lstlisting}

\begin{table}[H]
\begin{tabular}{r|l}
Method & Returns \\ \hline
\li{<<abs>>()}     & Object with absolute values taken (of numerical data) \\
\li{idxmax()}  & The index label of the maximum value \\
\li{idxmin()}  & The index label of the minimum value \\
\li{count()}   & The number of non-null entries \\
\li{cumprod()} & The cumulative product over an axis \\
\li{cumsum()}  & The cumulative sum over an axis \\
\li{<<max>>()}     & The maximum of the entries \\
\li{mean()}    & The average of the entries \\
\li{median()}  & The median of the entries \\
\li{<<min>>()}     & The minimum of the entries \\
\li{mode()}    & The most common element(s) \\
\li{prod()}    & The product of the elements \\
\li{<<sum>>()}     & The sum of the elements \\
\li{var()}     & The variance of the elements \\
\end{tabular}
\caption{Numerical methods of the \li{Series} and \li{DataFrame} pandas classes.
% Methods marked with a * are not methods of NumPy's \li{ndarray} class.
}
\label{table:pandas-numerical-methods}
\end{table}



\li{Series} are more useful than NumPy arrays when dealing with data primarily because of their index.
For example, a \li{Series} can be indexed by time with a pandas \li{DatetimeIndex}, an index with date and/or time values.
The usual way to create this kind of index is with \li{pd.date_range()}.

\begin{lstlisting}
# Make an index of the first t11	`hree days in July 2000.
>>> pd.date_range("7/1/2000", "7/3/2000", freq='D')
<<DatetimeIndex(['2000-07-01', '2000-07-02', '2000-07-03'],
                dtype='datetime64[ns]', freq='D')>>
\end{lstlisting}


\end{comment}

\begin{comment}
\begin{info}
The \li{Series} in Problem \ref{prob:pandas-random-walk} is an example of a \emph{time series}, since it is indexed by time.
Time series show up often in data science; we will explore them in more depth in another lab.
\end{info}
\end{comment}


\begin{comment}
A \li{DataFrame} is a collection of \li{Series} that share the same index, and is therefore a two-dimensional generalization of a NumPy array.
The row labels are collectively called the \emph{index}, and the column labels are collectively called the \emph{columns}.
An individual column in a \li{DataFrame} object is one \li{Series}.

There are many ways to initialize a \li{DataFrame}.
In the following code, we build a \li{DataFrame} out of a dictionary of \li{Series}.

\begin{lstlisting}
>>> x = pd.Series(np.random.randn(4), ['a', 'b', 'c', 'd'])
>>> y = pd.Series(np.random.randn(5), ['a', 'b', 'd', 'e', 'f'])
>>> df1 = pd.DataFrame({"series 1": x, "series 2": y})
>>> df1
<<   series 1  series 2
a -0.365542  1.227960
b  0.080133  0.683523
c  0.737970       NaN
d  0.097878 -1.102835
e       NaN  1.345004
f       NaN  0.217523>>
\end{lstlisting}

Note that the index of this \li{DataFrame} is the union of the index of \li{Series} \li{x} and that of \li{Series} \li{y}.
The columns are given by the keys of the dictionary \li{d}.
Since \li{x} doesn't have a label \li{e}, the
value in row \li{e}, column \li{1} is \li{NaN}.
This same reasoning explains the other missing values as well.
Note that if we take the first column of the \li{DataFrame} and drop the missing values, we recover the \li{Series} \li{x}:

\begin{lstlisting}
>>> df1["series1"].dropna()
<<a   -0.365542
b    0.080133
c    0.737970
d    0.097878
Name: series 1, dtype: float64>>
\end{lstlisting}



We can also initialize a \li{DataFrame} using a NumPy array, creating custom
row and column labels.
\begin{lstlisting}
>>> data = np.random.random((3, 4))
>>> pd.DataFrame(data, index=['A', 'B', 'C'], columns=np.arange(1, 5))

            1	        2	        3	        4
A	 0.065646	 0.968593	 0.593394	 0.750110
B	 0.803829	 0.662237	 0.200592	 0.137713
C	 0.288801	 0.956662	 0.817915	 0.951016
3 rows     4 columns
\end{lstlisting}

As with Series, if we don't specify the index or columns, the default is
\li{np.arange(n)}, where \li{n} is either the number of rows or columns.

\begin{comment} % Not necessary here. Too much info.
It is also possible to create multi-indexed arrays, for example:
\begin{lstlisting}
>>> grade=['eighth', 'ninth', 'tenth']
>>> subject=['math', 'science', 'english']
>>> myindex = pd.MultiIndex.from_product([grade, subject], names=['grade', 'subject'])
>>> myseries = pd.Series(np.random.randn(9), index=myindex)
>>> myseries
grade   subject
eighth  math       1.706644
        science   -0.899587
        english   -1.009832
ninth   math       2.096838
        science    1.884932
        english    0.413266
tenth   math      -0.924962
        science   -0.851689
        english    1.053329
dtype: float64
\end{lstlisting}

Multi-indexing is visually convenient, and will be explored further in Lab
\ref{lab:pandas3}, where we will discuss pandas pivot tables.
\end{comment}



\begin{comment} % Moved random walk to Series section.
\subsection*{Plotting}

Plotting is often a much more effective way to view and gain understanding of a dataset than simply viewing the raw numbers.
Fortunately, pandas interfaces well with matplotlib, allowing relatively
painless data visualization.

Most of the functionality of plotting using pandas will be discussed in Lab \ref{lab:pandas2}.
 In this lab, we will simply show how to plot a \li{Series}.
Doing so is easy, as the \li{Series} object is equipped with its own plot
function.
% Change this example to use the titanic data, which will be cleaned up.

Let's start with visualizing a simple random walk.
By way of background, a \emph{random walk} is a
stochastic process used to model a non-deterministic path through some space.
It is a useful construct in
many fields and can be used to explain things like the motion of a molecule as it travels through a liquid to modeling the fluctuations
of stock prices.
Here we will simulate a one-dimensional symmetric random walk on the integers, which can be
described as follows.
\begin{enumerate}
  \item Start at 0.
  \item Flip a fair coin.
  \item If heads, move one unit to the right.
Otherwise, move one unit to the left.
  \item Go to Step 2.
\end{enumerate}
How can we simulate this random walk efficiently? Note that the walk is really characterized by the outcomes
of the coin flip.
If we represent heads by the number $1$ and tails by $-1$, then our position at a given moment
is just the cumulative sum of all previous outcomes.
Below, we simulate a sequence of coin flips, build the
resulting random walk, and plot the outcome.
\begin{lstlisting}
>>> import matplotlib.pyplot as plt
>>> N = 1000        # length of random walk
>>> s = np.zeros(N)
>>> s[1:] = np.random.binomial(1, .5, size=(N-1,))*2-1 #coin flips
>>> s = pd.Series(s)
>>> s = s.cumsum()  # random walk
>>> s.plot()
>>> plt.ylim([-50, 50])
>>> plt.show()
>>> plt.close()
\end{lstlisting}

The random walk is shown in Figure \ref{fig:PandasRandomWalk}.

\begin{figure}
\centering
\includegraphics[width=.7 \textwidth]{randomWalk.pdf}
\caption{Random walk of length 1000.}
\label{fig:PandasRandomWalk}
\end{figure}

\begin{problem}
Create five random walks of length 100, and plot them together in the same plot.

Next, create a ``biased'' random walk by changing the coin flip probability of head from 0.5 to 0.51.
Plot this biased walk with lengths 100, 10000, and then 100000.
Notice the definite trend that emerges.
Your results should be comparable to those in Figure \ref{pandas:biasedRandomWalk}.
\end{problem}

\begin{figure}
\centering
\includegraphics[width=.7 \textwidth]{biasedRandomWalk.pdf}
\caption{Biased random walk of length 100 (above) and 10000 (below).}
\label{pandas:biasedRandomWalk}
\end{figure}
\end{comment}

\section*{SQL Operations in pandas} % =========================================

\li{DataFrames} are tabular data structures bearing an obvious resemblance to a typical relational
database table.
SQL is the standard for working with relational databases; however, pandas can accomplish many of the same tasks as SQL.
The SQL-like functionality of pandas is
one of its biggest advantages, eliminating the need to switch between programming languages
for different tasks.
Within pandas, we can handle both the querying \emph{and} data analysis.

For the examples below, we will use the following data:
\begin{lstlisting}
>>> name = ['Mylan', 'Regan', 'Justin', 'Jess', 'Jason', 'Remi', 'Matt', 'Alexander', 'JeanMarie']
>>> sex = ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F']
>>> age = [20, 21, 18, 22, 19, 20, 20, 19, 20]
>>> rank = ['Sp', 'Se', 'Fr', 'Se', 'Sp', 'J', 'J', 'J', 'Se']
>>> ID = range(9)
>>> aid = ['y', 'n', 'n', 'y', 'n', 'n', 'n', 'y', 'n']
>>> GPA = [3.8, 3.5, 3.0, 3.9, 2.8, 2.9, 3.8, 3.4, 3.7]
>>> mathID = [0, 1, 5, 6, 3]
>>> mathGd = [4.0, 3.0, 3.5, 3.0, 4.0]
>>> major = ['y', 'n', 'y', 'n', 'n']
>>> studentInfo = pd.DataFrame({'ID': ID, 'Name': name, 'Sex': sex, 'Age': age, 'Class': rank})
>>> otherInfo = pd.DataFrame({'ID': ID, 'GPA': GPA, 'Financial_Aid': aid})
>>> mathInfo = pd.DataFrame({'ID': mathID, 'Grade': mathGd, 'Math_Major': major})
\end{lstlisting}

Before querying our data, it is important to know some of its basic properties,
such as number of columns, number of rows, and the datatypes of the columns.
This can be done by simply calling the \li{info()} method on the desired
\li{DataFrame}:

\begin{lstlisting}
>>> mathInfo.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 5 entries, 0 to 4
Data columns (total 3 columns):
Grade         5 non-null float64
ID            5 non-null int64
Math_Major    5 non-null object
dtypes: float64(1), int64(1), object(1)
\end{lstlisting}

\begin{comment}
We can also get some basic information about the structure of the \li{DataFrame}
using the \li{head()} or \li{tail()} methods.

\begin{lstlisting}
>>> mathInfo.head()
   Grade  ID Math_Major  ID  Age  GPA
0    4.0   0          y   0   20  3.8
1    3.0   1          n   2   18  3.0
2    3.5   5          y   4   19  2.8
3    3.0   6          n   6   20  3.8
4    4.0   3          n   7   19  3.4
\end{lstlisting}
\end{comment}

SQL SELECT statements can be done by column indexing.
WHERE statements can be included by adding masks (just like in a NumPy array).
The method \li{isin()} can also provide a useful WHERE statement.
This method accepts a list, dictionary, or \li{Series} containing possible values of the \li{DataFrame} or \li{Series}.
When called upon, it returns a \li{Series} of booleans, indicating whether an entry contained a value in the parameter pass into \li{isin()}.


\begin{lstlisting}
>>> # SELECT ID, Age FROM studentInfo
>>> studentInfo[['ID', 'Age']]
   ID  Age
0   0   20
1   1   21
2   2   18
3   3   22
4   4   19
5   5   20
6   6   20
7   7   19
8   8   29

>>> # SELECT ID, GPA FROM otherInfo WHERE Financial_Aid = 'y'
>>> mask = otherInfo['Financial_Aid'] == 'y'
>>> otherInfomask][['ID', 'GPA']]
   ID  GPA
0   0  3.8
3   3  3.9
7   7  3.4

>>> # SELECT Name FROM studentInfo WHERE Class = 'J' OR Class = 'Sp'
>>>  studentInfo[studentInfo['Class'].isin(['J','Sp'])]['Name']
0        Mylan
4        Jason
5         Remi
6         Matt
7    Alexander
Name: Name, dtype: object
\end{lstlisting}

\begin{comment}
\begin{problem}
Using panda commands, find the crime rates of the years where Property crimes were less than 90$\%$ of the total crime and burglary crimes were more than 20$\%$ of the total crime or property crimes were less than 90$\%$ of the total crime while violent crimes were more than 12$\%$ of the total crime.
\end{problem}
\end{comment}

Next, let's look at JOIN statements.
In pandas, this is done with the \li{merge} function.
\li{merge} takes the two \li{DataFrame} objects to join as parameters, as well as keyword arguments specifying
the column on which to join, along with the type (left, right, inner, outer).

\begin{lstlisting}
>>> # SELECT * FROM studentInfo INNER JOIN mathInfo ON studentInfo.ID = mathInfo.ID
>>> pd.merge(studentInfo, mathInfo, on='ID') # INNER JOIN is the default
   Age Class  ID    Name Sex  Grade Math_Major
0   20    Sp   0   Mylan   M    4.0          y
1   21    Se   1   Regan   F    3.0          n
2   22    Se   3    Jess   F    4.0          n
3   20     J   5    Remi   F    3.5          y
4   20     J   6    Matt   M    3.0          n
[5 rows x 7 columns]

>>> # SELECT GPA, Grade FROM otherInfo FULL OUTER JOIN mathInfo ON otherInfo.ID = mathInfo.ID
>>> pd.merge(otherInfo, mathInfo, on='ID', how='outer')[['GPA', 'Grade']]
   GPA  Grade
0  3.8    4.0
1  3.5    3.0
2  3.0    NaN
3  3.9    4.0
4  2.8    NaN
5  2.9    3.5
6  3.8    3.0
7  3.4    NaN
8  3.7    NaN
[9 rows x 2 columns]
\end{lstlisting}

\begin{problem}
Read in the file \li{crime_data.csv} as a pandas object.
The file contains data on types of crimes in the U.S. from 1960 to 2016.
Set the index as the column 'Year'.

Create a new column \li{Rate} which contains the crime rate for each year.
Using panda commands, find the number of murders in the years where the crime rate was greater than 5\% and the number of \li{Violent} was more than on average.
Return an array containing the number of murders in these years.

(Hint: To do \li{AND} statements, use two masks. Use \li{values} attribute to create array.)
\label{prob:rate}
\end{problem}

\begin{problem}
Answer the following questions using the file \li{crime_data.csv} and the pandas methods learned in this lab.
The answer of each question should be saved as indicated.
Return the answers to each question as a tuple (i.e. \li{(answer_1,answer_2,answer_3)}).

\begin{enumerate}
	\item Identify the three crimes that have a mean over 1,500,000. 
	Of these three crimes, which two are very correlated? 
	Which of these two crimes has a greater maximum value?
	Save the title of this column as a variable to return as the answer.
	\item Examine the data since 2000.
	Sort this data (in ascending order) according to number of murders.
	SELECT Aggravated Assault WHERE Aggravated Assault is greather than 850,000.
	Save the reordered and SQL queried \li{DataFrame} as a NumPy array to return as the answer.
	\item What decade had the most crime? 
	In this decade, which crime was committed the most? 
	What percentage of the total crime that year was it? 
	Save this value as a float.
\end{enumerate}
\end{problem}

\begin{comment}
\begin{problem}
Read in the files \li{final_accidents2.pickle} and \li{final_drivers.pickle} using the function \li{read_pickle()}.  
The accidents file contains data on various accidents that occurred in the U.S. and their causes.
The drivers data has data on various tickets given to drivers, how many tickets given, and what the ticket is for.
The following table shows the meanings of the codes in the data.

\begin{table}[H]
\begin{tabular}{r|l}
Code & Description \\ \hline
\li{describe()}  & Return a \li{Series} describing the data structure \\
\li{head()}      & Return the first $n$ rows, defaulting to 5 \\
\li{tail()}      & Return the last $n$ rows, defaulting to 5 \\

\li{DRUNK_DR} & 0-99 - Number of drunk drivers \\ \hline
\li{WEATHER} & 0 - Unknown \\
             & 1 - Clear \\
             & 2 - Rain \\
             & 3 - Sleet, Hail \\
             & 4 - Snow \\
             & 5 - Fog, Smog, Smoke \\
             & 6 - Severe Crosswinds \\
             & 7 - Blowing Sand, Soil, and Dirt \\
             & 8 - Other \\
             & 10 - Cloudy \\
             & 11 - Blowing Snow \\
             & 12 - Freezing Rain or Drizzle \\
             & 98 - Not Reported \\
             & 99 - Unknown \\ \hline
\li{SP} & 0 - Not Speeding \\
        & 1 - Speeding \\
        & 8 - Not Reported \\
        & 9 - Unknown \\ \hline
\li{FATALS} & 0-99 - Number of fatalities \\ \hline
\li{VEH_NO} & Assigned number of motor vehicle \\ \hline
\li{DRINKING} & 0 - Alcohol Not Involved \\ 
              & 1 - Alcohol Involved \\
              & 8 - Not Reported \\
              & 9 - Unknown \\ \hline
\li{SPEEDREL} & 0 - No \\ 
              & 1 - Yes \\
              & 2 - Yes, Racing \\
              & 3 - Yes, Exceeded Speed Limit \\
              & 4 - Yes, Too Fast for Conditions \\
              & 5 - Yes, Specifics Unknown \\
              & 8 - No Driver Present/Unknown if Driver Present \\
              & 9 - Unknown

\end{tabular}
\caption{Codes for \li{FARS} data.}
\label{table:fars-codes}
\end{table}

Create a \li{DataFrame} containing the Case Number, Age, whether the accident was caused by a drunk driver, whether the driver had DUIs, and if the accident was fatal, using an inner join operation.
Create a second \li{DataFrame} containing Case Number, Age, and fatalities using an inner join operation. In this \li{DataFrame}, also include the speed column from each dataframe.

Use these dataframes to answer the following questions.
Make sure to support your claims with appropriate and presentable data.

\begin{enumerate}
\item What is the most common kind of weather involved with accidents?

\item How many accidents involved speeding?

\item Of accidents where the type of speeding is known, what is the most common type of speeding in accidents?

\item Rounding to the nearest integer, what is the mean number of drunk drivers involved in each accident?

\item What is the mean number of fatalities per accident each year?

\item What month has the most fatalities each year?
\end{enumerate}
\end{problem}
\end{comment}

\begin{comment}
This may be going into too much detail for an introductory lab...
It is sometimes desirable to join to DataFrames on their indexes rather than on a particular column.
The \li{join} method makes this operation convenient.
\begin{lstlisting}
>>> #create new DataFrame
>>> sibs = [0, 1, 0, 5, 2, 9]
>>> sibInfo = pd.DataFrame({'Siblings':sibs})
>>> sibInfo
   Siblings
0         0
1         1
2         0
3         5
4         2
5         9
[6 rows x 1 columns]

>>> #now join studentInfo with sibInfo
>>> studentInfo.join(sibInfo)
   Age Class  ID    Name Sex  Siblings
0   20    Sp   0    Mylan   M         0
1   21    Se   1   Regan   F         1
2   18    Fr   2     Justin   M         0
3   22    Se   3   Jess   F         5
4   19    Sp   4     Jason   M         2
5   20     J   5  Remi   F         9
6   20     J   6   Matt   M       NaN
7   19     J   7  Alexander   M       NaN
8   20    Se   8     JeanMarie   F       NaN
[9 rows x 6 columns]
\end{lstlisting}
The default is a left join, but this can be altered.
When attempting to join to DataFrames that share common columns on their indexes,
we must rename the columns to avoid contradictions.
We do this by simply appending
characters to the existing column names, specified by the \li{lsuffix} and \li{rsuffix}
parameters (for the left and right DataFrames, respectively).
\begin{lstlisting}
>>> #join studentInfo and mathInfo on indexes
>>> studentInfo.join(mathInfo, lsuffix='_left', rsuffix='_right', how='inner')
   Age Class  ID_left   Name Sex  Grade  ID_right Math_Major
0   20    Sp        0   Mylan   M    4.0         0          y
1   21    Se        1  Regan   F    3.0         1          n
2   18    Fr        2    Justin   M    3.5         5          y
3   22    Se        3  Jess   F    3.0         6          n
4   19    Sp        4    Jason   M    4.0         3          n
[5 rows x 8 columns]
\end{lstlisting}

The final SQL-like operation we will discuss is the UNION, or concatenation of DataFrames.
In the simplest setting, this operation is useful when we have two DataFrames that have the
same columns but possibly different rows.
\begin{lstlisting}
>>> #create another df holding more math info
>>> mathID2 = [0, 5, 2, 4]
>>> mathGd2 = [4.0, 3.5, 2.0, 3.0]
>>> major2 = ['y', 'y', 'n', 'y']
>>> mathInfo2 = pd.DataFrame({'Grade':mathGd2, 'ID':mathID2, 'Math_Major':major2})
>>> pd.concat([mathInfo, mathInfo2], ignore_index=True).drop_duplicates()
   Grade  ID Math_Major
0    4.0   0          y
1    3.0   1          n
2    3.5   5          y
3    3.0   6          n
4    4.0   3          n
7    2.0   2          n
8    3.0   4          y
[7 rows x 3 columns]
\end{lstlisting}
The \li{ignore_index=True} argument means we discard the indexes of the two input DataFrames and create
a new one for the concatenated DataFrame.
The \li{drop_duplicates} method simply drops duplicate rows.
\end{comment}




\begin{comment}
\begin{problem}
In 1912 the RMS \emph{Titanic} sank after colliding with an iceberg.
The file \texttt{titanic.csv} contains data on the incident.
Each row represents a different passenger, and the columns describe various features of the passengers (age, sex, whether or not they survived, etc.)

Start by cleaning the data.
\begin{itemize}
    \item Read the data into a \li{DataFrame}.
    Use the first row of the file as the column labels, but do not use any of the columns as the index.
    \item Drop the columns \li{"Sibsp"}, \li{"Parch"}, \li{"Cabin"}, \li{"Boat"}, \li{"Body"}, and \li{"home.dest"}.
    \item Drop any entries without data in the \li{"Survived"} column, then change the remaining entries to \li{True} or \li{False} (they start as 1 or 0).
    \item Replace null entries in the \li{"Age"} column with the average age.
    \item Save the new \li{DataFrame} as \texttt{titanic\_clean.csv}.
\end{itemize}
Next, answer the following questions.
\begin{itemize}
    \item How many people survived? What percentage of passengers survived?
    \item What was the average price of a ticket? How much did the most expensive ticket cost?
    \item How old was the oldest survivor? How young was the youngest survivor? What about non-survivors?
\end{itemize}



\end{problem}
\end{comment}
